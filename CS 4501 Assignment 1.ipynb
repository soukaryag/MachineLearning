{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Comparison of Multivariate Linear Regression Models\n",
    "## Soukarya Ghosh (sg4fz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS 4501 Machine Learning - Department of Computer Science - University of Virginia\n",
    "*In this assignment, you will get to pick your favorite data set on Kaggle (https://kaggle.com) and implement some learning models in the context of a **regression** problem.  For references, you may refer to [my slides](https://docs.google.com/presentation/d/10D1he89peAWaFgjtZlHpUzvOOAie_vIFT95htKCKgc0/edit#slide=id.p) or Chapter 4 of the textbook if you need additional sample codes to help with your assignment. To get started, you will need to determine which dataset to download and copy it into the directory where you wish to run your implementation (ie. same folder as this file).* \n",
    "\n",
    "*For deliverables, you must write code in Python and submit **this** Jupyter Notebook file (.ipynb) to earn a total of 100 pts. You will gain points depending on how you perform in the following sections.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1. BIG PICTURE (5 pts)\n",
    "Write a paragraph explaining the context of the problem in which you are trying to investigate. In the same paragraph, explain why you pick your dataset. Then, you MUST include the URL to the dataset to help the TA to download it (we will assume that the data file is put into the same folder as the Jupyter Notebook file). Then, you can write some code to load the CSV file and take a quick look at the dataset, and output the following:\n",
    "\n",
    " * How big is your dataset? (in terms of MB)\n",
    "     * **The dataset is 8 kb or 0.008 mb**\n",
    " * How many entries does it have?\n",
    "     * **This dataset has a total of 1000 total rows**\n",
    " * How many features does it have?\n",
    "     * **There are a total of 8 columns that will be used as features. The target column can be made into any of the three test scores. After fully pipelining te data with onehotencoder, the number of columns in the matrix will be 19 total.**\n",
    " * What are some basic statistics you can learn right away about this dataset?\n",
    "     * **Data suggests that as score for one test goes up, the scores for the other tests goes up as well**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some common packages\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "# Your code goes here for this section, make sure you also include the output to answer the above questions.\n",
    "datasetURL = \"https://www.kaggle.com/spscientist/students-performance-in-exams\"; # required url to download for CSV file. The TA will need to download the file and run your program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 8 columns):\n",
      "gender                         1000 non-null object\n",
      "race/ethnicity                 1000 non-null object\n",
      "parental level of education    1000 non-null object\n",
      "lunch                          1000 non-null object\n",
      "test preparation course        1000 non-null object\n",
      "math score                     1000 non-null int64\n",
      "reading score                  1000 non-null int64\n",
      "writing score                  1000 non-null int64\n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 62.6+ KB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "EDUCATION_PATH = \"StudentsPerformance.csv\"\n",
    "\n",
    "def load_education_data(education_path=EDUCATION_PATH):\n",
    "    return pd.read_csv(EDUCATION_PATH)\n",
    "\n",
    "education = load_education_data()\n",
    "education.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "math score       1.000000\n",
       "reading score    0.817580\n",
       "writing score    0.802642\n",
       "Name: math score, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = education.corr()\n",
    "corr_matrix[\"math score\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>66.08900</td>\n",
       "      <td>69.169000</td>\n",
       "      <td>68.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.16308</td>\n",
       "      <td>14.600192</td>\n",
       "      <td>15.195657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>57.00000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>57.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>66.00000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>69.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>77.00000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>79.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.00000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       math score  reading score  writing score\n",
       "count  1000.00000    1000.000000    1000.000000\n",
       "mean     66.08900      69.169000      68.054000\n",
       "std      15.16308      14.600192      15.195657\n",
       "min       0.00000      17.000000      10.000000\n",
       "25%      57.00000      59.000000      57.750000\n",
       "50%      66.00000      70.000000      69.000000\n",
       "75%      77.00000      79.000000      79.000000\n",
       "max     100.00000     100.000000     100.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "education.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. DATA DISCOVERY, CLEANING, AND SCALING (10 pts)\n",
    "\n",
    "**Data Discover:** Plot out all correlations among the features. You may notice some features are more correlated with your predicted value than other. This information will help you confirm that weights of your regression model later on.\n",
    "\n",
    "**Data Cleaning:** If your dataset has some missing values, make sure you are able to fill those values with the Imputer class. If your dataset has categorical features, make sure you conver those features into numerical using OneHotEncoder class. \n",
    "\n",
    "**Feature Scaling** More importantly, your task is to write some codes to normalize the value of each features as follow:\n",
    "\n",
    "* Subtract the mean value of each feature from the dataset\n",
    "* Scale (divide) the feature values by their respective standard deviation\n",
    "\n",
    "Implementation Note: You will do this for all features and your code should work with datasets of all sizes (any number of features/ examples). After learning the parameters from the model, you must first normalize the new $x$ value using the mean and standard deviation that you have previously computed from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#education.hist(bins=50, figsize=(20,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(education, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "education = train_set.drop(\"math score\", axis=1)\n",
    "#education_temp = education_temp.drop(\"reading score\", axis=1)\n",
    "#education = education_temp.drop(\"writing score\", axis=1)\n",
    "education_labels = train_set[\"math score\"].copy()        #only testing math score for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\souka\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "c:\\users\\souka\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "education_num = education.drop('gender', axis=1)\n",
    "education_num = education_num.drop('race/ethnicity', axis=1)\n",
    "education_num = education_num.drop('parental level of education', axis=1)\n",
    "education_num = education_num.drop('lunch', axis=1)\n",
    "education_num = education_num.drop('test preparation course', axis=1)\n",
    "num_attribs = list(education_num)\n",
    "cat_attribs = [\"gender\", \"race/ethnicity\", \"parental level of education\", \"lunch\", \"test preparation course\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs)\n",
    "    ])\n",
    "\n",
    "\n",
    "education_prepared = (full_pipeline.fit_transform(education))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(education_prepared, education_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.248303471123559"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "education_predictions = lin_reg.predict(education_prepared)\n",
    "lin_mse = mean_squared_error(education_labels, education_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2988071523335984"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(education_prepared, education_labels)\n",
    "education_predictions = tree_reg.predict(education_prepared)\n",
    "tree_mse = mean_squared_error(education_labels, education_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\souka\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\pipeline.py:451: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    }
   ],
   "source": [
    "# You might want to use the following package\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.plotting import scatter_matrix # optional\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Your code goes here for this section.\n",
    "X_train = education_prepared\n",
    "y_train = education_labels\n",
    "\n",
    "\n",
    "X_test_temp = test_set.drop(\"math score\", axis=1)\n",
    "#X_test_temp = X_test_temp.drop(\"reading score\", axis=1)\n",
    "#X_test_temp = X_test_temp.drop(\"writing score\", axis=1)\n",
    "X_test_prepared = (full_pipeline.transform(X_test_temp))\n",
    "\n",
    "\n",
    "X_test = X_test_prepared\n",
    "y_test = test_set[\"math score\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 3. IMPLEMENTATION OF GRADIENT DESCENT (45 pts)\n",
    "The gradient descent formulation remain the same as one in the lecture. Keep in mind that you will need to add a column $\\textbf{x}_0$ with all 1s as part of the training data. You should write code to implement the **MyLinearRegression** class and its predefined methods. \n",
    "\n",
    "* **Gradient Descent:** Notes that you may NOT call the library linear regression which defeats the purpose of this assignment. Make sure your code supports any number of features and is well-vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Error: [14.96516438]\n"
     ]
    }
   ],
   "source": [
    "# NOTE: You may not use the library Linear Regression, but implement your own!\n",
    "# REMEMBER to place self.attribute = 0 with value from your implementation\n",
    "import numpy as np\n",
    "\n",
    "class MyLinearRegression:\n",
    "    def __init__(self):\n",
    "        self.theta = np.random.randn(20,1); # parameter vector;\n",
    "        self.alpha = 0; # learning rate\n",
    "        self.cost  = []; # cost function\n",
    "  \n",
    "    def fitUsingGradientDescent(self, X_train, y_train):\n",
    "        # implementation code here\n",
    "        X_b = np.insert(X_train, 0, 1, axis=1)\n",
    "        self.alpha = 0.0001\n",
    "        self.theta, cost = self.gradientDescent(X_b, y_train.values, self.theta, self.alpha, 1)\n",
    "        lowestCost = cost\n",
    "        for oof in range(100):\n",
    "            tempT, tempCost = self.gradientDescent(X_b, y_train.values, self.theta, self.alpha, oof)\n",
    "            if tempCost < lowestCost:\n",
    "                self.theta = tempT\n",
    "        \n",
    "        # print(self.theta)\n",
    "    \n",
    "    \n",
    "    def costCalc(self, theta, X_train, y_train):\n",
    "        m = len(y_train)\n",
    "        predictions = X_train.dot(theta)\n",
    "        cost = (1/2*m)*np.sum(np.square(predictions-y_train))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def gradientDescent(self, X_train, y_train, theta, alpha, iters):\n",
    "        # INPUT:\n",
    "        # alpha: the learning rate\n",
    "        # iters: number of iterations\n",
    "        # \n",
    "        # OUTPUT: \n",
    "        # theta: updated value for theta\n",
    "        # cost: value of the cost function\n",
    "        #\n",
    "        # implementation code here;\n",
    "\n",
    "        eta = alpha # learning rate\n",
    "        n_iterations = iters\n",
    "        m = len(y_train)\n",
    "        y_train = y_train.reshape(700,1)\n",
    "        \n",
    "        for iteration in range(n_iterations):\n",
    "            gradients = 2/m * X_train.T.dot(X_train.dot(theta) - y_train)\n",
    "            theta = theta - eta * gradients\n",
    "            \n",
    "        costT = self.costCalc(theta, X_train, y_train)\n",
    "        self.cost.append(costT)\n",
    "\n",
    "        return theta, costT\n",
    "\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        # implementation code here \n",
    "        X_b = np.insert(X_test, 0, 1, axis=1)\n",
    "        y_predict = X_b.dot(self.theta)\n",
    "\n",
    "        return y_predict\n",
    "    \n",
    "    \n",
    "    def fitUsingNormalEquation(self, X_train, y_train):\n",
    "        # implementation code here for PART 4.\n",
    "        X_b = np.insert(X_train, 0, 1, axis=1)\n",
    "        y = y_train.values.reshape(700,1)\n",
    "        \n",
    "        self.theta = np.linalg.pinv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) \n",
    "        \n",
    "        #print(self.theta)\n",
    "        \n",
    "    \n",
    "# Your code goes here to call the instance of class MyLinearRegression\n",
    "myGradientDescentModel = MyLinearRegression()\n",
    "myGradientDescentModel.fitUsingGradientDescent(X_train, y_train)\n",
    "a = myGradientDescentModel.predict(X_test)\n",
    "sum = 0\n",
    "#print(a)\n",
    "# #print(y_train.values)\n",
    "for x in range(300):\n",
    "    sum += abs(a[x] - y_train.values[x])\n",
    "    #print(a[x], y_test.values[x])\n",
    "    \n",
    "print(\"Avg Error: \" + str(sum/300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Learning Rate:** You will try out different learning rates for the dataset and find a learning rate that converges quickly. If you pick a learning rate, your plot of Cost Function $J(\\theta)$ against number of iteration will quickly decay to a small value. This also indicates that your implementation is correct. If your learning rate is too large, the cost function $J(\\theta)$ can diverge and blow up. From the below plot, you must be able to report the best learning rate the you found to earn credit for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Error vs. Training Iterations')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the following code to plot out your learning rate\n",
    "# iters and cost must be supplied to plot out the cost function\n",
    "# You may plot multiple curves corresponding to different learning rates to justify the best one.\n",
    "\n",
    "myGradientDescentModel = MyLinearRegression()\n",
    "myGradientDescentModel.fitUsingGradientDescent(X_train, y_train)\n",
    "a = myGradientDescentModel.predict(X_test)\n",
    "\n",
    "\n",
    "plt.plot(list(range(len(myGradientDescentModel.cost))), myGradientDescentModel.cost, \"r-\")\n",
    "plt.xlabel('Iterations')  \n",
    "plt.ylabel('Cost')  \n",
    "plt.title('Error vs. Training Iterations')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 4. IMPLEMENTATION OF THE NORMAL EQUATION (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my lecture, you learn that the closed form solution of linear regression using the normal equation formulation. Using the formula does not require any feature scaling, and should be straight forward to implement: \n",
    "\n",
    "$\n",
    "    \\mathbf{\\theta} = ({\\mathbf{X}^{T}\\mathbf{X}})^{-1}\\mathbf{X}^{T}\\mathbf{y}.\n",
    "$\n",
    "\n",
    "Note that you still need to add a column of 1's to the $\\mathbf{\n",
    "X}$ matrix to have an intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Error: [16.57880486]\n"
     ]
    }
   ],
   "source": [
    "# Implement the normalEquation method of the MyLinearRegression Class before execute the code below:\n",
    "myNormalEquationModel = MyLinearRegression()\n",
    "myGradientDescentModel.fitUsingNormalEquation(X_train, y_train)\n",
    "normalfit = myGradientDescentModel.predict(X_test)\n",
    "#print(normalfit)\n",
    "\n",
    "sum = 0\n",
    "for x in range(300):\n",
    "    sum += abs(normalfit[x] - y_train.values[x])\n",
    "    \n",
    "print(\"Avg Error: \" + str(sum/300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 5. COMPARISON OF DIFFERENT IMPLEMENTATIONS (20 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to evaluate and compare your gradient descent as well as normal equation implementation of linear regression. In theory, they should be the same, or at least similar. For good measures, you may also use the built-in library **Scholastic Gradient Descent (SGD)** as a third model for comparison. For each model, you must compute the Root Mean Squared Error (RMSE) as performance measure. The good news is that you can call library functions to compute these as shown below instead of writing your own code:\n",
    "\n",
    "* Which one yields the best performance measure for your dataset?\n",
    "    * **The Normal Equation implementation has the lowest error as shown by the bar graph below. Although it is only a little better than the scikit learn baked SGD.**\n",
    "* What is your assessment of the error? Good? Okay? Terrible?\n",
    "    * **This error is a little bit underwhelming. Since these are test scores with stdev of about 15 for math (shown in the first section), the error is nearly one standard deviation away from the actual score the certain student would receive. This is not very accurate for our purposes and there would hopefully need to be a better model to fit this case. While it is 'ok' for ballparking, it can not be used to make any actual predictions or analysis of the features utilized, i.e. preparation, parent education, etc. I would think decision tree would be the best for this situation of modelling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD MSE: 5.672107813561309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\souka\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myGradientDescentMode MSE: 8.710068641614837\n",
      "myNormalEquationMode MSE: 5.557570799195738\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Use the built-in SGD Regressor model\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "mySGDModel = SGDRegressor()\n",
    "mySGDModel.fit(X_train,y_train)\n",
    "y_predict = mySGDModel.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_predict)\n",
    "mySGDModel_rmse = np.sqrt(mse)\n",
    "print(\"SGD MSE: \" + str(mySGDModel_rmse))\n",
    "\n",
    "#myGradientDescentModel_rmse\n",
    "\n",
    "myGradientDescentModel = MyLinearRegression()\n",
    "myGradientDescentModel.fitUsingGradientDescent(X_train, y_train)\n",
    "y_predict_mgd = myGradientDescentModel.predict(X_test)\n",
    "mse_mgd = mean_squared_error(y_test, y_predict_mgd)\n",
    "myGDModel_rmse = np.sqrt(mse_mgd)\n",
    "print(\"myGradientDescentMode MSE: \" + str(myGDModel_rmse))\n",
    "\n",
    "#myNormalEquationModel_rmse \n",
    "\n",
    "myNormalEquationModel = MyLinearRegression()\n",
    "myGradientDescentModel.fitUsingNormalEquation(X_train, y_train)\n",
    "y_predict_ne = myGradientDescentModel.predict(X_test)\n",
    "mse_ne = mean_squared_error(y_test, y_predict_ne)\n",
    "myNEModel_rmse = np.sqrt(mse_ne)\n",
    "print(\"myNormalEquationMode MSE: \" + str(myNEModel_rmse))\n",
    "\n",
    "\n",
    "x = np.arange(3)\n",
    "errors = [mySGDModel_rmse, myGDModel_rmse, myNEModel_rmse]\n",
    "barlist = plt.bar(x, errors)\n",
    "plt.xticks(x, ('SGD (scikit)', 'MyGD', 'MyNE'))\n",
    "plt.xlabel('Mean Squared Error')  \n",
    "plt.title('MSE by Regression Model') \n",
    "barlist[0].set_color('r')\n",
    "plt.axhline(y=mySGDModel_rmse, linewidth=1, color='k', linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 6. PRESENTATION OF YOUR SOLUTION (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you need to write a short memo of one paragraph to be read by a non-technical audience (ie. your manager/boss). Focus on answering the following: \n",
    "\n",
    "* How can you pitch your solution to this project? \n",
    "* What did you learn so far about the problem?\n",
    "* Is there any insight moving forward to improve the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I would pitch the solution to this project as a proof of concept, showing that there exists a trend that can be \\napproximated by self evaluated preparation (confidence), parental education, demographic info, and two other exam scores.\\nBased on these criterias, if a school is able to identify a group of students as being more likely to succeed/fail on an\\nexam, they can direct resources appropriately to make sure those who are struggling can be ultimately successful. I have\\nlearned so far that the most impactful variables seem be the scores of the other exams, ideally we would want to slash \\nthese from the list as well, but for the purposes of this project, I wanted to see how they affected the overall fitting.\\nIn terms of knowledge gained, I now have a much better understanding of the whole process instead of just copy-pasting\\ncode. I can better understand what gd is doing under the hood and this will be invaluable going forward. In order to\\nimprove the solution, I believe that it will be better to scrap the use of gradient descent and look for a better algorithm,\\nsuch as decision tree. Since the categorical data is almost always binary, this lends itself well to a decision tree\\nand could yield a lower rmse. I hope to revisit this project once I have more tools under my belt, I want to be the error\\ndown to 5% within the actual test scores, not using any of the other test scores as features. If I could do that, then\\nthere could potentially be some real impactful followups being done here.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your paragraph goes here for this section\n",
    "\"\"\"I would pitch the solution to this project as a proof of concept, showing that there exists a trend that can be \n",
    "approximated by self evaluated preparation (confidence), parental education, demographic info, and two other exam scores.\n",
    "Based on these criterias, if a school is able to identify a group of students as being more likely to succeed/fail on an\n",
    "exam, they can direct resources appropriately to make sure those who are struggling can be ultimately successful. I have\n",
    "learned so far that the most impactful variables seem be the scores of the other exams, ideally we would want to slash \n",
    "these from the list as well, but for the purposes of this project, I wanted to see how they affected the overall fitting.\n",
    "In terms of knowledge gained, I now have a much better understanding of the whole process instead of just copy-pasting\n",
    "code. I can better understand what gd is doing under the hood and this will be invaluable going forward. In order to\n",
    "improve the solution, I believe that it will be better to scrap the use of gradient descent and look for a better algorithm,\n",
    "such as decision tree. Since the categorical data is almost always binary, this lends itself well to a decision tree\n",
    "and could yield a lower rmse. I hope to revisit this project once I have more tools under my belt, I want to be the error\n",
    "down to 5% within the actual test scores, not using any of the other test scores as features. If I could do that, then\n",
    "there could potentially be some real impactful followups being done here.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - \n",
    "### NEED HELP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you get stuck in any step in the process, you may find some useful information from:\n",
    "\n",
    " * Consult my [slides](https://docs.google.com/presentation/d/10D1he89peAWaFgjtZlHpUzvOOAie_vIFT95htKCKgc0/edit?usp=sharing) and/or the textbook\n",
    " * Talk to the TA, they are available and there to help you during [office hour](http://bit.ly/cs4501oh)\n",
    " * Come talk to me or email me <nn4pj@virginia.edu> with subject starting \"CS4501 Assignment 1:...\".\n",
    "\n",
    "Best of luck and have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
